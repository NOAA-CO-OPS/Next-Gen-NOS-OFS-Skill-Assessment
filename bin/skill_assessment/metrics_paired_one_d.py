"""
 This is script takes the pandas dataframe generated by get_skill.py and
   calculates the skill assessment metrics. The output is a list with
   the result for each metric.
"""
import math

from sklearn.metrics import root_mean_squared_error
from scipy import stats
import numpy as np
from scipy.stats import ConstantInputWarning
import os
import warnings
import pandas as pd

warnings.simplefilter('ignore', ConstantInputWarning)

def get_error_range(name_var,prop,logger):

    filename = 'error_ranges.csv'
    filepath = os.path.join(prop.path,'conf',filename)
    if os.path.isfile(filepath) is True:
        # Dataframe of error range file!
        df = pd.read_csv(filepath)
        subs = df[df["name_var"]==name_var]

    else:
        # Make file using default values
        errordata = [['salt',3.5,0.5],['temp',3,0.5],['wl',0.15,0.5],
                     ['cu',0.26,0.5]]
        df = pd.DataFrame(errordata, columns=['name_var','X1','X2'])
        subs = df[df["name_var"]==name_var]
        df.to_csv(filepath, index=False)

    # Get error ranges for variable
    X1 = pd.to_numeric(subs["X1"]).values[0]
    X2 = pd.to_numeric(subs["X2"]).values[0]

    return X1, X2

def skill_scalar(df_paired, name_var, prop, logger):
    """
    some stations have constant salinity (or current direction,
    e.g. all USGS currents stations will have this problem) for the entire
    period you are running
    By definition you cannot calculate the correlation between
    observations and model if one of the timeseries (array) is constant.
    So, that is what the warning is saying
    """
    datathreshold = 5

    # obs = df_paired["OBS"]
    # ofs = df_paired["OFS"]
    # df_bias = df_paired["BIAS"]

    #Get target error range
    X1, X2 = get_error_range(name_var,prop,logger)

    df_paired = df_paired.dropna(subset=["OBS", "OFS"])
    # Update obs and ofs after handling NaN
    obs = df_paired["OBS"]
    ofs = df_paired["OFS"]
    df_bias = df_paired["BIAS"]

    if np.nansum(~np.isnan(obs)) >= datathreshold:
        # RMSE -- fixed 8/13/24
        rmse = root_mean_squared_error(obs, ofs)
        rmse = np.around(rmse, decimals=2)

        # Pearson's R
        r_value = stats.pearsonr(obs, ofs)[0]
        if math.isnan(r_value):
            logger.warning(
                '%s -- The correlation coefficient could not be calculated (i.e. R=NaN)', name_var)
        r_value = np.around(r_value, decimals=2)

        # Mean bias & bias percent
        bias = df_bias.mean() #Series mean from fortran SA
        bias = np.around(bias, decimals=2)
        bias_perc = 100 * (bias / obs.mean())
        bias_perc = np.around(bias_perc, decimals=2)

        # Central frequency
        # Not using X2 right now, only X1
        npbias = np.array(df_bias)
        cf = ((((-X1 <= npbias) & (npbias <= X1)).sum())/len(npbias))*100
        cf = np.around(cf, decimals=2)
        # Pass or fail?
        if cf >= 90:
            cfpf = 'pass'
        else:
            cfpf = 'fail'

        # Positive/negative outlier frequency
        pof = ((((2*X1 <= npbias)).sum())/len(npbias))*100
        pof = np.around(pof, decimals=2)
        if pof <= 1:
            pofpf = 'pass'
        else:
            pofpf = 'fail'
        nof = ((((npbias <= -2*X1)).sum())/len(npbias))*100
        nof = np.around(nof, decimals=2)
        if nof <= 1:
            nofpf = 'pass'
        else:
            nofpf = 'fail'

        # Standard deviation or error/bias
        stdev = np.std(npbias)
        stdev = np.around(stdev,decimals=2)

    else:
        nodatatext = '<' + str(datathreshold) + ' data points'
        rmse = nodatatext
        r_value = nodatatext
        bias = nodatatext
        bias_perc = nodatatext
        cf = nodatatext
        cfpf = nodatatext
        pof = nodatatext
        pofpf = nodatatext
        nof = nodatatext
        nofpf = nodatatext
        stdev = nodatatext

    # Return stats
    return [rmse,
            r_value,
            bias,
            bias_perc,
            np.nan,
            cf,
            cfpf,
            pof,
            pofpf,
            nof,
            nofpf,
            stdev,
            X1]


def skill_vector(df_paired, name_var, prop, logger):
    '''
    skill_vector
    '''
    datathreshold = 5

    # obs = df_paired["OBS"]
    # ofs = df_paired["OFS"]
    # spd_bias = df_paired["SPD_BIAS"]

    # OBS_DIR = df_paired['OBS_DIR']
    # OFS_DIR = df_paired['OFS_DIR']
    dir_bias = df_paired["DIR_BIAS"]

    # Get target error range
    X1, X2 = get_error_range(name_var,prop,logger)

    df_paired = df_paired.dropna(subset=["OBS", "OFS"])
    # Update obs and ofs after handling NaN
    obs = df_paired["OBS"]
    ofs = df_paired["OFS"]
    spd_bias = df_paired["SPD_BIAS"]
    dir_bias = df_paired["DIR_BIAS"]
    if np.nansum(~np.isnan(obs)) >= datathreshold:
        # RMSE -- fixed 8/13/24
        rmse = root_mean_squared_error(obs, ofs)
        rmse = np.around(rmse, decimals=2)

        # Pearson's R
        r_value = stats.pearsonr(obs, ofs)[0]
        if math.isnan(r_value):
            logger.warning(
                '%s -- The correlation coefficient could not be calculated (i.e. R=NaN)',
                name_var)
        r_value = np.around(r_value, decimals=2)

        # Mean bias & bias percent
        bias = spd_bias.mean()
        bias_perc = 100 * (bias / obs.mean())
        bias = np.around(bias, decimals=2)
        bias_perc = np.around(bias_perc, decimals=2)
        bias_dir = dir_bias.mean()
        bias_dir = np.around(bias_dir, decimals=2)
        # bias_dir =np.around(bias_dir, decimals=3)
        # bias_dir_perc =np.around(bias_dir_perc, decimals=3)

        # Central frequency
        # Not using X2 right now, only X1
        npbias = np.array(spd_bias)
        cf = ((((-X1 <= npbias) & (npbias <= X1)).sum())/len(npbias))*100
        cf = np.around(cf, decimals=2)
        # Pass or fail?
        if cf >= 90:
            cfpf = 'pass'
        else:
            cfpf = 'fail'

        # Positive/negative outlier frequency
        pof = ((((2*X1 <= npbias)).sum())/len(npbias))*100
        pof = np.around(pof, decimals=2)
        if pof <= 1:
            pofpf = 'pass'
        else:
            pofpf = 'fail'
        nof = ((((npbias <= -2*X1)).sum())/len(npbias))*100
        nof = np.around(nof, decimals=2)
        if nof <= 1:
            nofpf = 'pass'
        else:
            nofpf = 'fail'

        # Standard deviation or error/bias
        stdev = np.std(npbias)
        stdev = np.around(stdev,decimals=2)
    else:
        nodatatext = '<' + str(datathreshold) + ' data points'
        rmse = nodatatext
        r_value = nodatatext
        bias = nodatatext
        bias_perc = nodatatext
        bias_dir = nodatatext
        cf = nodatatext
        cfpf = nodatatext
        pof = nodatatext
        pofpf = nodatatext
        nof = nodatatext
        nofpf = nodatatext
        stdev = nodatatext

    return [rmse,
            r_value,
            bias,
            bias_perc,
            bias_dir,
            cf,
            cfpf,
            pof,
            pofpf,
            nof,
            nofpf,
            stdev,
            X1]
